<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>MoE(Mixture of Experts)——By Liu Xin (51255903045)</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="MoE(Mixture of Experts)——By Liu Xin (51255903045)">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="MoE(Mixture of Experts)——By Liu Xin (51255903045)">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Liuxin_51255903045 (ifshine)">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="MoE(Mixture of Experts)——By Liu Xin (51255903045)" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">MoE(Mixture of Experts)——By Liu Xin (51255903045)</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-总结" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/04/03/%E6%80%BB%E7%BB%93/" class="article-date">
  <time class="dt-published" datetime="2023-04-02T22:50:33.227Z" itemprop="datePublished">2023-04-03</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/04/03/%E6%80%BB%E7%BB%93/">总结</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="博客主题：对混合专家（Mixture-of-Experts，简称MoE）的技术总结"><a href="#博客主题：对混合专家（Mixture-of-Experts，简称MoE）的技术总结" class="headerlink" title="博客主题：对混合专家（Mixture-of-Experts，简称MoE）的技术总结"></a>博客主题：对混合专家（Mixture-of-Experts，简称MoE）的技术总结</h1><p>选取原因：该技术泛用性好</p>
<h1 id="博客页面布局及其设计思路"><a href="#博客页面布局及其设计思路" class="headerlink" title="博客页面布局及其设计思路"></a>博客页面布局及其设计思路</h1><p>全部遵循默认的Hexo的布局和设计</p>
<h1 id="博客功能实现及其技术选择"><a href="#博客功能实现及其技术选择" class="headerlink" title="博客功能实现及其技术选择"></a>博客功能实现及其技术选择</h1><p>功能主要是展示MoE的相关论文介绍，技术上是选择Hexo框架</p>
<h1 id="博客样式设计及其美学考量"><a href="#博客样式设计及其美学考量" class="headerlink" title="博客样式设计及其美学考量"></a>博客样式设计及其美学考量</h1><p>全部遵循默认的Hexo的样式设计及其美学考量</p>
<h1 id="博客制作过程中遇到的问题及其解决方法"><a href="#博客制作过程中遇到的问题及其解决方法" class="headerlink" title="博客制作过程中遇到的问题及其解决方法"></a>博客制作过程中遇到的问题及其解决方法</h1><ol>
<li>问题1<ul>
<li>【问题】刚开始不知道选择哪种框架能够快速搭建博客</li>
<li>【解决办法】试了几个框架，觉得Hexo是最合适的</li>
</ul>
</li>
<li>问题2<ul>
<li>【问题】配置环境的过程中遇到一些小bug</li>
<li>【解决办法】在网上搜索别人的相关经验</li>
</ul>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/04/03/%E6%80%BB%E7%BB%93/" data-id="clg00fj6900040stk258k22kd" data-title="总结" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-混合专家" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/03/21/%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6/" class="article-date">
  <time class="dt-published" datetime="2023-03-21T06:31:52.356Z" itemprop="datePublished">2023-03-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/03/21/%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6/">混合专家</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <ul>
<li><strong>混合专家的定义</strong>：混合专家（Mixture-of-Experts，简称MoE）是包含若干专家（expert）模块和至少一个门控（gate）模块的网络结构，专家模块给出预测结果，门控模块判断每个专家的重要性。<ul>
<li><strong>稀疏专家模型的定义</strong>：（Sparse Expert Models）模型部分参数被划分为若干”专家”，每个专家维护不同的参数；在训练和测试阶段，由路由模块（router）将输入样本路由到特定的专家（们），最终每个样本只与模型的<strong>部分参数</strong>交互。</li>
</ul>
</li>
<li><strong>混合专家的发展历程</strong>：1）深度学习时代以前（1991-2012）。<a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf">1991</a>年最早提出MoE，每个专家模块是一个完整的神经网络，门控模块数量为1，MoE类似集成学习方法。<a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~hinton/absps/hme.pdf">1994</a>年提出了分层MoE（Hierarchical MoE，简称HMoE），它是树状结构，叶子结点是专家模块，非叶子结点是门控模块，HMoE是模块化思想的发展。<a target="_blank" rel="noopener" href="https://web.archive.org/web/20170830033215id_/http://www.ee.hacettepe.edu.tr/~eyuksel/Publications/2012_TwentyYearsofMixtureofExperts.pdf">2012</a>年对MoE进行综述，阐述了MoE能灵活地与各种模型结合（分类、回归等）。2）深度学习时代（2012-至今）。许多工作研究MoE在稀疏专家模型上的设计，以扩大模型规模或提高模型的可扩展性。<a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=B1ckMDqlg">2017</a>年将MoE作为一个组件加入到LSTM模型中，训练出当时最大的模型，达到语言建模、机器翻译任务的SOTA。2020年的<a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=qrwe7XHTmYb">Gshard</a>和2021年的<a target="_blank" rel="noopener" href="https://www.jmlr.org/papers/volume23/21-0998/21-0998.pdf">Switch Transformer</a>将MoE组件引入到Transformer中，重新引起研究人员对MoE的兴趣；除了NLP，2021年开始MoE在CV、Speech Recognition、Multi-Modal领域都有了成功的尝试。</li>
<li><strong>混合专家的问题场景</strong>：<ol>
<li><strong>计算有限下扩大模型规模</strong>。现在的模型越来越大，训练样本越来越多，每个样本都需要经过模型的全部计算，这就导致了训练成本的平方级增长。MoE将大模型拆分成多个小模型（expert），对于一个样本来说，无需经过所有的小模型去计算，而只是激活一部分小模型进行计算，这样就节省了计算资源。需要注意的是，计算量虽然没有过多增加，但是模型的参数量非常大，内存占用是一个负担，有可能减少batchsize。<ol>
<li>有许多GPU。非常适合稀疏模型。通常使用数据并行训练模型，每个GPU只用负责一部分数据(而不是全部)，省出来的资源可以托管更多模型参数。当预训练阶段使用多个GPU、微调阶段的GPU少得多，稀疏度(专家的数量)需要修改以匹配下游任务的内存限制。</li>
<li>GPU数量少。一般情况下，稀疏模型总是比密集模型看起来更差(一般受内存限制)。不过一些巧妙合适的设计也可以让MoE发挥作用，见问题场景的第2、3点。</li>
</ol>
</li>
<li><strong>可扩展性</strong>。2022的<a target="_blank" rel="noopener" href="https://aclanthology.org/2022.naacl-main.407.pdf">DEMIX</a>和<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2208.03306">BTM</a>重新审视了专家的模块化、可组合性，后面有DEMIX的详细介绍。</li>
<li><strong>多样化生成内容</strong>。2022的<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.07285.pdf">MoKGE</a>在常识解释生成任务上，让每个专家选择sub KG上不同的结点，从而生成不同的常识解释。</li>
</ol>
</li>
</ul>
<h2 id="稀疏专家模型的未来方向"><a href="#稀疏专家模型的未来方向" class="headerlink" title="稀疏专家模型的未来方向"></a><strong>稀疏专家模型的</strong>未来方向</h2><ul>
<li><strong>自适应计算（Adaptive Computation）</strong>：自适应计算的思想是，对不同的输入，模型的计算量不同（即时调整）。稀疏模型有类似的想法，每个输入使用相同的计算量，但激活的参数不同。这2个技术并不互斥。一些routing算法分配给一个token的expert数量是不同的；模型也可以自动选择需要使用的layer数量；异构expert layer（type、size）也会带来计算量的不同。未来可以继续用自适应计算促进稀疏专家模型的发展。</li>
<li><strong>检索方法（Retrieval Methods）</strong>：检索机制可以有效扩充模型容量，它能让模型动态获取外部信息。稀疏专家模型与检索模型有一个共同目标：扩充模型容量以更好地存储、检索、运用知识，不过前者通过增加参数量来实现该目标。研究这2个技术的结合很可能是个有潜力的方向。</li>
<li><strong>Many Open Questions Remain</strong>：<ul>
<li>对专家的最佳<strong>数量和规模</strong>如何取决于任务仍然知之甚少</li>
<li>实现强大的<strong>领域外泛化</strong>并不那么简单，需要更好的解释</li>
<li>大多数稀疏专家模型的<strong>结构多样性</strong>相对较低</li>
<li>仍然必须确定合适的<strong>稀疏粒度</strong>：大多数工作都集中在专家替换组件（如前馈网络层），不过DEMIX和BTM发现了更完全模块化的独立专家的好处</li>
<li>有关稀疏专家模型的一些特性还未被理解：记忆能力，异步训练下的表现，etc.</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/03/21/%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6/" data-id="clg00fj6900050stk4tgy5jmt" data-title="混合专家" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Gshard" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/03/21/Gshard/" class="article-date">
  <time class="dt-published" datetime="2023-03-21T06:30:05.621Z" itemprop="datePublished">2023-03-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/03/21/Gshard/">Gshard</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Author: Google<br>Contribution: k≤2随机路由, 引入MoE到Transformer<br>[Venue] Year: ICLR 2021</p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=qrwe7XHTmYb"></a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/lucidrains/mixture-of-experts">https://github.com/lucidrains/mixture-of-experts</a></p>
<ul>
<li>第一个把MoE结构引入Transformer结构的工作。与2017的工作的不同点：sparse gating函数、auxiliary loss<ul>
<li>novel gating function<ul>
<li><strong>平衡负载：</strong>引入fixed expert capacity概念(2017是设计了loading balancing函数)，相比之下前者有助gating函数的并行化实现</li>
<li><strong>local dispatching</strong>: 在一个batch中，每个expert的总capacity是相同且固定的。把该batch的N个token均等分为G组，<strong>每组的gating计算是并行的</strong>，每个expert将它的capacity均等分配到每组；因而组数越多，每组得到的capacity越少，溢出概率越大；组数越少，每组得到的capacity越多，溢出概率越小，但gating并行计算数量越少，模型吞吐量也越小</li>
<li>$k\le2$, <strong>random routing</strong>: GATE(·) dispatches to the 2nd-best expert with the probability proportional to its weight g2, 减少溢出、提高性能</li>
</ul>
</li>
<li>Gshard Annotation<ul>
<li>代码编写模型的时候，开发人员感觉像在单一设备(有巨大的内存和计算容量)上；而实际上是多个设备，编译器根据代码中的少量Gshard Annotation，自动划分计算量到每个硬件设备</li>
<li>TPU, TensorFlow</li>
</ul>
</li>
<li>条件计算</li>
<li>训练稳定性<ul>
<li>using bfloat16 activations with a 1 trillion parameter model遇到不稳定现象</li>
<li>采取的解决措施：using higher precision (float32)(但这也需要占用更多的内存，训练速度变慢)</li>
</ul>
</li>
</ul>
</li>
<li>举个机器翻译(中—&gt;英)的例子：<ul>
<li>假设我们有三个专家，他们分别擅长翻译主语，谓语，宾语</li>
<li>input: “我爱你”(3个token，每个token可以被认为是一个单词)</li>
<li>分发器把第一个token发给主语专家，第二个发给谓语专家，第三个发给宾语专家，分别得到”I”, “love”, “you”</li>
<li>出错情况：把第一个token也发给了宾语专家，那可能最后翻译的结果就是”me love you”了</li>
</ul>
</li>
<li>实验：多语言机器翻译</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/tensorflow/lingvo#running-the-gshard-transformer-based-giant-language-model">GitHub - tensorflow&#x2F;lingvo: Lingvo</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/03/21/Gshard/" data-id="clg00fj6700020stk27gu17um" data-title="Gshard" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Switch Transformer" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/03/21/Switch%20Transformer/" class="article-date">
  <time class="dt-published" datetime="2023-03-21T06:28:53.616Z" itemprop="datePublished">2023-03-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/03/21/Switch%20Transformer/">Switch Transformer</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Author: Google<br>Contribution: k&#x3D;1<br>[Venue] Year: JMLR 2022</p>
<p><a target="_blank" rel="noopener" href="https://www.jmlr.org/papers/volume23/21-0998/21-0998.pdf"></a></p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/model_doc/switch_transformers">SwitchTransformers</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/tree/67d074874d285e616393c65a0e670088e1b6b74a/src/transformers/models/switch_transformers">transformers&#x2F;src&#x2F;transformers&#x2F;models&#x2F;switch_transformers at 67d074874d285e616393c65a0e670088e1b6b74a · huggingface&#x2F;transformers</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/switch/__init__.py">annotated_deep_learning_paper_implementations&#x2F;<strong>init</strong>.py at master · labmlai&#x2F;annotated_deep_learning_paper_implementations</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/344702054">NLP炼丹笔记：Switch Transformers 朴实无华 大招秒杀</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/363702998">动态路由条件计算-万亿级参数超大模型关键技术与MindSpore支持（1）</a></p>
<ol>
<li>Switch Transformer虽然有1.6万亿参数，但通过<strong>Sparse routing</strong>的改进，每轮迭代只会触发部分Expert的计算，而每个token也只会路由给一个Expert，所以对算力的需求并没有随着参数量的增加而大幅增长，使得这个模型更加容易训练(缓解不稳定性)</li>
<li>数据并行、模型并行、Expert并行的并行策略设计，在MoE网络结构上能够获得更低的通信开销，提高并行的效率</li>
</ol>
<ul>
<li>this work advocates training large models on <strong>relatively small amounts of data</strong> as the computationally optimal approach</li>
<li>Our work here focuses on TPU architectures, but these class of models may be similarly trained on GPU clusters</li>
<li>每个设备上的权重不同，因而模型权重数量随着设备数量的增加而增加</li>
<li>2017年提出的Mixture of Expert Routing: 在N个expert中选择前k个(2017认为k&gt;1比较好)<ul>
<li>路由器维护一个可学习的矩阵$W_r$, 先计算logits $h(x)&#x3D;W_r·x$，然后再计算softmax分布，第i个expert分配到的gate值为$p_i(x)&#x3D;\frac{e^{h(x)_i}}{\sum_j^Ne^{h(x)_j}}$(gate值能反映路由器的鉴别能力)</li>
<li>令$\mathcal T$是选出来的k个expert的下标集合，每个expert的计算结果为$E_i(x)$, 则模型最终输出这k个expert计算结果的线性加权和，$y&#x3D;\sum\limits_{i\in\mathcal T}p_i(x)E_i(x)$</li>
</ul>
</li>
<li>Switch transformer设置k&#x3D;1，有3个好处：<ol>
<li>减少计算量</li>
<li>减少batch size(expert capacity), 因为每个expert处理的token数量减少(至少减少了一半)</li>
<li>路由算法的实现被简化，通信代价减少(见下图)</li>
</ol>
</li>
</ul>
<p>红线对应的token不会被当前层处理(溢出)。较低比率的溢出不会影响模型的性能。expert capacity—the number of tokens each expert computes。capacity factor需要权衡溢出比率和计算代价。capacity factor是一个超参？</p>
<ul>
<li>2017 separates load-blancing and importance-weighting losses</li>
<li>Switch Transformer 使用 <strong>Differentiable Load Balancing Loss</strong><ul>
<li>For each Switch layer, this auxiliary loss is added to the total model loss during training.</li>
<li>Given N experts and a batch $\mathcal B$ with $T$ tokens, the loss is computed as the scaled dot-product between $f$ and $P$<ul>
<li>$loss &#x3D; \alpha ·N\sum\limits_{i&#x3D;1}^Nf_i·P_i$</li>
<li>We swept hyper-parameter ranges of α from 10−1 to 10−5 in powers of 10 and found 10−2 <strong>balanced load quickly</strong> without interfering with training loss</li>
<li>$f_i&#x3D;\frac{1}{T}\sum\limits_{x\in\mathcal B}1\left{\mathrm{argmax}\space p(x)&#x3D;i\right}$, 分配到第i个expert的token数量与总token数量(T)的比值</li>
<li>$P_i&#x3D;\frac{1}{T}\sum\limits_{x\in\mathcal B}p_i(x)$, 第i个expert的gate值的平均(每个token对应一个gate值，共T个gate值)</li>
</ul>
</li>
<li>该loss能鼓励uniform routing：尽量让每个expert都分配到差不多数量的token</li>
</ul>
</li>
<li>随意增加expert的数量有可能让模型效果变差</li>
</ul>
<ol>
<li>数据并行：n&#x3D;N, m&#x3D;1<ol>
<li>优点：在整个前向、反向传播结束前，没有通信开销；在所有前向、反向传播结束后，需要将不同core上的模型梯度进行聚合</li>
</ol>
</li>
<li>模型并行：n&#x3D;1, m&#x3D;N<ol>
<li>通信开销大(每轮计算完前向、反向传播后，需要聚合梯度)</li>
<li>Each core sends a tensor of $[B, d_{model}]$</li>
</ol>
</li>
<li>模型和数据并行：In the forward and backward pass each core communicates a tensor of size $[\frac{B}{n}, d_{model}]$ in an all-reduce operation</li>
<li>专家和数据并行：For each token per core a router locally computes assignments to the experts</li>
<li>专家、模型和数据并行：</li>
</ol>
<ul>
<li>训练稳定性<ul>
<li>countered instabilities in the highest-compute Switch-XXL model</li>
<li>采取的解决办法：推荐使用尺寸更小的权重初始化，将路由网络的部分参数设置为更高的精度</li>
</ul>
</li>
<li>Future Work<ol>
<li>最大模型的训练稳定性需要进一步提高(正则化器&#x2F;梯度裁剪变体)</li>
<li>微调质量、FLOPS per token、参数数量，这3者间的依赖目前还没有很好的认识</li>
<li>对scaling relationship还需进行一个全面研究，以对融合数据、模型、专家平行的架构设计进行指导</li>
<li>本文的工作包含在自适应计算范畴，使用的是同构的expert，在未来可以设计异构expert</li>
<li>探索除了Transformer FFN层以外的其他layer，作为expert layer。</li>
<li>将Switch Transformer从NLP领域扩展到多模态领域</li>
</ol>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/03/21/Switch%20Transformer/" data-id="clg00fj6800030stk6x5sc1jt" data-title="Switch Transformer" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-DEMIX" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/03/21/DEMIX/" class="article-date">
  <time class="dt-published" datetime="2023-03-21T06:27:15.623Z" itemprop="datePublished">2023-03-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/03/21/DEMIX/">DEMIX</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Author: Meta, 华盛顿大学<br>Contribution: 专家的模块化、可组合性<br>[Venue] Year: NAACL 2022</p>
<p>DEMIX Layers: Disentangling Domains for Modular Language Modeling</p>
<p><a target="_blank" rel="noopener" href="https://aclanthology.org/2022.naacl-main.407.pdf"></a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/kernelmachine/demix">https://github.com/kernelmachine/demix</a></p>
<ul>
<li>在routing上与Gshard比较<ul>
<li>Gshard解决的问题是，扩展模型规模面临高昂的训练代价；而模块化任务不需要面临该问题</li>
<li>Gshard缺乏模块化，暗示它在下游任务上面临与密集模型相似的问题<ul>
<li>在adaptation的过程中，遗忘已学习到的知识，缺乏轻量级的控制</li>
</ul>
</li>
<li>DEMIX尽管在模型规模上远小于Gshard，但它尤其在分布外的表现要优于Gshard。这暗示在提高模型泛化能力的过程中，除了扩大模型规模，领域模块化也是一个代替解决方案</li>
</ul>
</li>
<li>DEMIX experts are related to <strong>adapters</strong> (Bapna and Firat, 2019), which <strong>add a small feedforward network into a frozen pretrained LM to enable parameter efficient finetuning</strong>. In contrast, our focus is on efficiently training all of the parameters of a modular LM from scratch, and as such is not directly comparable to existing adapter schemes. Adapters could enable more fine-grained control over which parts of the LM are domain-specific, and may circumvent the need to train domain-aware LMs from scratch. However, the shared parameters in the frozen pretrained LM may limit modularity. We leave exploring such architectural variants and their tradeoffs to future work.</li>
<li>每个专家专注于一个领域，让语言模型变得模块化，在推理阶段<ol>
<li><strong>mixing</strong> experts采用无参数的加权集成，有助泛化到<strong>异构领域、新领域</strong>(训练阶段未看到)。如COVID-19 papers</li>
<li>可以<strong>添加</strong>expert: 以适应新领域；不会遗忘原有的expert。如Github code<ul>
<li>DEMIX-DAPT—cheap adaptation without forgetting</li>
<li>initialize a new expert in each DEMIX layer using the parameters of the nearest pretrained expert(根据前面提到的，参数无关的、计算出来的gate值)</li>
</ul>
</li>
<li>可以<strong>删除</strong>expert: 对不想涉及的领域，切断对它的访问。如social media</li>
</ol>
</li>
<li>关于数据标记<ul>
<li>在训练阶段为每个样本<strong>做domain标记</strong>，每个expert负责一个domain，显式路由</li>
<li>在测试阶段，每个样本<strong>没有domain标记</strong>，模型计算每个expert的gate值<ul>
<li>mix expert</li>
<li>参数无关; 仅根据输入token来计算</li>
<li>而Gshard中，需要学习含参数的权重矩阵，来计算这些gate值</li>
</ul>
</li>
</ul>
</li>
<li>未来工作：Future work may combine domain and token-level routing, discover domains automatically with <strong>unsupervised learning</strong>, or scale the number of training domains.</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/03/21/DEMIX/" data-id="clg00fj6500010stk1l7p5a12" data-title="DEMIX" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">April 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/03/">March 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/04/03/%E6%80%BB%E7%BB%93/">总结</a>
          </li>
        
          <li>
            <a href="/2023/03/21/%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6/">混合专家</a>
          </li>
        
          <li>
            <a href="/2023/03/21/Gshard/">Gshard</a>
          </li>
        
          <li>
            <a href="/2023/03/21/Switch%20Transformer/">Switch Transformer</a>
          </li>
        
          <li>
            <a href="/2023/03/21/DEMIX/">DEMIX</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 Liuxin_51255903045 (ifshine)<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>